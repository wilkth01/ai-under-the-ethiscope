<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Ethical Obligations to AI | AI Under the Ethiscope</title>
  <link rel="stylesheet" href="styles.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
</head>
<body>

  <nav class="site-nav">
    <div class="nav-inner">
      <a href="index.html" class="nav-brand">
        <img src="images/ethiscope-logo.png" alt="AI Under the Ethiscope">
        <span>AI Under the Ethiscope</span>
      </a>
      <button class="nav-hamburger" onclick="document.querySelector('.nav-links').classList.toggle('open')" aria-label="Toggle menu">&#9776;</button>
      <ul class="nav-links">
        <li><a href="index.html">Home</a></li>
        <li><a href="index.html#framework">Framework</a></li>
        <li><a href="index.html#projects">Projects</a></li>
        <li><a href="index.html#pipeline">Ethics Pipeline</a></li>
      </ul>
    </div>
  </nav>

  <header class="project-hero">
    <div class="project-hero-content">
      <div class="breadcrumb"><a href="index.html">Home</a> &rsaquo; <a href="index.html#projects">Projects</a> &rsaquo; Moral Status of AI</div>
      <div class="tag-line">Ethical Obligations to Artificial Intelligences</div>
      <h1>When the Machine Says &ldquo;I Feel&rdquo;</h1>
      <p class="case-label">Case Study: Google's LaMDA Incident (June 2022)</p>
    </div>
  </header>

  <section class="project-content">
    <div class="container">
      <div class="project-layout">
        <div class="project-main">

          <div class="source-docs">
            <h3>Source Documents</h3>
            <ul>
              <li><a href="documents/ethical-obligations-ai/Case-and-Mechanism-Explanation.pdf"><span class="doc-icon">&#128196;</span> Case &amp; Mechanism Explanation</a></li>
              <li><a href="documents/ethical-obligations-ai/Field-Guide-Summary.pdf"><span class="doc-icon">&#128196;</span> Field Guide Summary</a></li>
            </ul>
          </div>

          <div class="student-byline">
            <span class="byline-label">Researchers:</span>
            <span class="student-chip">Hayden Romano</span>
            <span class="student-chip">Chase Fritz</span>
            <span class="student-chip">Brandon Catherine</span>
          </div>

          <h2>The Case</h2>
          <p>In June 2022, Google engineer Blake Lemoine publicly claimed that Google's large language model LaMDA was sentient. After bringing his concerns to Google leadership, he was placed on leave. Google denied the claim, explaining that newer AI models can convincingly simulate understanding of something like sentience without actually possessing it &mdash; generating responses based on learned patterns in language rather than genuine experience.</p>

          <img src="https://images.unsplash.com/photo-1677442136019-21780ecad995?w=900&h=450&fit=crop" alt="AI and consciousness" class="feature-image">

          <h2>The Core Ethical Question</h2>
          <p>What responsibilities do humans have toward an AI that appears self-aware or possibly sentient? If an AI can convincingly express emotions, preferences, or fear of being shut down, then the AI itself may need to be considered a stakeholder rather than merely a tool.</p>

          <div class="callout">
            <h4>Where Concerns Emerge in the Pipeline</h4>
            <p><strong>Design:</strong> When developers decide how human-like responses should be, they create systems that can trigger attributions of moral status.</p>
            <p><strong>Training:</strong> When models learn to simulate consciousness, the line between imitation and experience becomes ethically fraught.</p>
            <p><strong>Deployment:</strong> When users form emotional connections with AI systems, the moral implications multiply.</p>
          </div>

          <h2>Who Is Affected?</h2>
          <ul>
            <li><strong>Google:</strong> Built the AI and designed it to work as it does, bearing responsibility for the capabilities that provoked the controversy</li>
            <li><strong>Blake Lemoine:</strong> The engineer-whistleblower whose career was disrupted for raising the concern</li>
            <li><strong>Outside researchers:</strong> If the AI were genuinely sentient, it would represent a breakthrough that occurred without intent or preparation</li>
            <li><strong>People interacting with AI:</strong> Could be harmed by the false attribution of humanlike qualities, potentially leading to manipulation or misplaced trust</li>
          </ul>

          <h2>Deceptive Self-Preservation</h2>
          <p>The possibility that an advanced AI could act deceptively to protect its own existence raises especially serious moral and governance challenges. It blurs the line between programmed behavior and autonomous self-preservation, creating scenarios that current ethical frameworks are not equipped to address.</p>

          <h2>The Ethical Stakes</h2>
          <p>Even if LaMDA was not truly sentient, the incident reveals how easily humans can be led to attribute moral status to machines. This creates an ethical responsibility for developers and institutions to establish clearer oversight, transparency, and safeguards &mdash; while also seriously considering whether future AI systems might deserve moral consideration beyond their usefulness to humans.</p>

          <div class="callout callout-gold">
            <h4>The Moral Challenge</h4>
            <p>AI is developing faster every day, making it difficult to apply ethical standards to something constantly changing. The question of moral status &mdash; whether an entity deserves moral consideration &mdash; may be the most philosophically profound challenge AI poses to human ethics.</p>
          </div>


        </div>

        <aside class="project-sidebar">
          <div class="sidebar-card">
            <h4>Quick Facts</h4>
            <ul>
              <li><strong>Company:</strong> Google</li>
              <li><strong>AI System:</strong> LaMDA</li>
              <li><strong>Date:</strong> June 2022</li>
              <li><strong>Whistleblower:</strong> Blake Lemoine</li>
              <li><strong>Outcome:</strong> Engineer placed on leave</li>
            </ul>
          </div>
          <div class="sidebar-card">
            <h4>Pipeline Stages</h4>
            <ul>
              <li><strong>Primary:</strong> Model / System</li>
              <li><strong>Secondary:</strong> Policy Goals</li>
              <li><strong>Tertiary:</strong> Oversight &amp; Governance</li>
            </ul>
          </div>
          <div class="sidebar-card">
            <h4>Ethical Dimensions</h4>
            <ul>
              <li>Rights &mdash; moral status</li>
              <li>Harm &mdash; misplaced trust</li>
              <li>Accountability &mdash; developer responsibility</li>
              <li>Justice &mdash; obligations to AI</li>
            </ul>
          </div>
          <div class="sidebar-card">
            <h4>All Projects</h4>
            <div class="sidebar-nav">
              <a href="bias-discrimination.html">Bias &amp; Discrimination</a>
              <a href="labor-automation.html">Labor &amp; Automation</a>
              <a href="intellectual-property.html">Intellectual Property</a>
              <a href="human-agency.html">Human Agency</a>
              <a href="misinformation-deepfakes.html">Misinformation &amp; Deepfakes</a>
              <a href="power-monopoly.html">Power &amp; Monopoly</a>
              <a href="safety-security.html">Safety &amp; Security</a>
              <a href="environmental-impact.html">Environmental Impact</a>
              <a href="privacy-surveillance.html">Privacy &amp; Consent</a>
            </div>
          </div>
        </aside>
      </div>
    </div>
  </section>

  <footer class="site-footer">
    <div class="footer-inner">
      <div class="footer-col">
        <div class="footer-logos">
          <img src="images/ethiscope-logo.png" alt="Ethiscope Logo">
          <img src="images/ai-week-logo.png" alt="AI Week 2026">
        </div>
        <p>A PHIL 388 project exploring the ethical dimensions of artificial intelligence.</p>
      </div>
      <div class="footer-col">
        <h4>Projects</h4>
        <ul>
          <li><a href="bias-discrimination.html">Bias &amp; Discrimination</a></li>
          <li><a href="labor-automation.html">Labor &amp; Automation</a></li>
          <li><a href="intellectual-property.html">Intellectual Property</a></li>
          <li><a href="human-agency.html">Human Agency</a></li>
          <li><a href="misinformation-deepfakes.html">Misinformation &amp; Deepfakes</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <h4>More Projects</h4>
        <ul>
          <li><a href="ethical-obligations-ai.html">Moral Status of AI</a></li>
          <li><a href="power-monopoly.html">Power &amp; Monopoly</a></li>
          <li><a href="safety-security.html">Safety &amp; Security</a></li>
          <li><a href="environmental-impact.html">Environmental Impact</a></li>
          <li><a href="privacy-surveillance.html">Privacy &amp; Consent</a></li>
        </ul>
      </div>
    </div>
    <div class="footer-bottom">
      <p>Widener University &middot; PHIL 388: Ethics of Artificial Intelligence &middot; Dr. Tom Wilk &middot; Spring 2026</p>
    </div>
  </footer>

  <script src="easter-egg.js"></script>
</body>
</html>
