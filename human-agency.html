<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Human Agency, Manipulation &amp; Autonomy | AI Under the Ethiscope</title>
  <link rel="stylesheet" href="styles.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
</head>
<body>

  <nav class="site-nav">
    <div class="nav-inner">
      <a href="index.html" class="nav-brand">
        <img src="images/ethiscope-logo.png" alt="AI Under the Ethiscope">
        <span>AI Under the Ethiscope</span>
      </a>
      <button class="nav-hamburger" onclick="document.querySelector('.nav-links').classList.toggle('open')" aria-label="Toggle menu">&#9776;</button>
      <ul class="nav-links">
        <li><a href="index.html">Home</a></li>
        <li><a href="index.html#framework">Framework</a></li>
        <li><a href="index.html#projects">Projects</a></li>
        <li><a href="index.html#pipeline">Ethics Pipeline</a></li>
      </ul>
    </div>
  </nav>

  <header class="project-hero">
    <div class="project-hero-content">
      <div class="breadcrumb"><a href="index.html">Home</a> &rsaquo; <a href="index.html#projects">Projects</a> &rsaquo; Human Agency</div>
      <div class="tag-line">Human Agency, Manipulation &amp; Autonomy</div>
      <h1>Engineered to Scroll</h1>
      <p class="case-label">Case Study: TikTok's Recommendation Algorithm</p>
    </div>
  </header>

  <section class="project-content">
    <div class="container">
      <div class="project-layout">
        <div class="project-main">

          <div class="source-docs">
            <h3>Source Documents</h3>
            <ul>
              <li><a href="documents/human-agency/Case-and-Mechanism-Explanation.pdf"><span class="doc-icon">&#128196;</span> Case &amp; Mechanism Explanation</a></li>
              <li><a href="documents/human-agency/Field-Guide-Summary.pdf"><span class="doc-icon">&#128196;</span> Field Guide Summary</a></li>
            </ul>
          </div>

          <div class="student-byline">
            <span class="byline-label">Researchers:</span>
            <span class="student-chip">Areanna Cantu</span>
            <span class="student-chip">Alyssa Schuebel <span class="student-school">&middot; Dwyer School of Nursing</span></span>
          </div>

          <h2>The Case</h2>
          <p>TikTok's recommendation algorithm shapes user experience by delivering a constant stream of short videos tailored to each user. The AI-driven system uses machine learning to analyze vast engagement data &mdash; watch time, shares, comments, and likes &mdash; to estimate what videos each user is most likely to engage with. This happens primarily on the "For You" page (FYP), the default feed where most users spend the majority of their time.</p>

          <img src="https://images.unsplash.com/photo-1611162617474-5b21e879e113?w=900&h=450&fit=crop" alt="Social media and smartphone use" class="feature-image">

          <h2>Three Decision Points in the Pipeline</h2>

          <h3>1. Data Collection</h3>
          <p>TikTok constantly collects behavioral data, including small interactions users may not recognize as impactful. Drawing on Shoshana Zuboff's surveillance capitalism framework, platforms extract behavioral data not just to predict user preferences but to shape future behavior for profit. Users do not always meaningfully consent to the scale of data extraction that powers personalization.</p>

          <h3>2. Model Optimization</h3>
          <p>The algorithm is designed to maximize engagement, especially retention and watch time. When engagement becomes the primary objective, the system learns to intensify addictive or emotionally stimulating content. This produces narrowed exposure to diverse perspectives, reinforcement loops, and habit conditioning that encourages prolonged use.</p>

          <h3>3. Deployment Design</h3>
          <p>Features like infinite scroll and autoplay remove natural stopping cues, intensifying the algorithm's influence. These interface decisions contribute to "doomscrolling" and unhealthy patterns of dependency. Long-term impacts disproportionately affect teenagers, who may experience diminished autonomy over their attention and increased mental health risks.</p>

          <div class="callout">
            <h4>The Ethical Problem</h4>
            <p>AI-driven recommendation systems make manipulative choices by algorithmically shaping attention and habits, especially affecting users' capacity for reflective choice and autonomy. The algorithm redirects behavior by nudging users toward content that optimizes engagement, exploiting psychological vulnerabilities like social validation, emotional reactivity, and susceptibility to addiction.</p>
          </div>

          <h2>The Autonomy Paradox</h2>
          <p>Users believe they are curating their own experience and "training" their algorithm, when in reality they are choosing from options pre-filtered and prioritized by AI systems optimized for corporate profit. People are treated as data sources for advertising revenue rather than as autonomous agents capable of reflective choice.</p>

          <div class="callout callout-gold">
            <h4>Key Ethical Questions</h4>
            <p>When do AI-driven nudges cross the moral line from assisting user choice to undermining autonomous decision-making, especially when users are unaware of how their behavior is being shaped?</p>
            <p>What counts as meaningful consent in AI-mediated environments where personalization is constant and opting out of algorithmic influence is difficult or unrealistic?</p>
          </div>


        </div>

        <aside class="project-sidebar">
          <div class="sidebar-card">
            <h4>Quick Facts</h4>
            <ul>
              <li><strong>Platform:</strong> TikTok</li>
              <li><strong>Feature:</strong> For You Page (FYP)</li>
              <li><strong>Technology:</strong> ML recommendation</li>
              <li><strong>Most affected:</strong> Teenagers</li>
              <li><strong>Key design:</strong> Infinite scroll, autoplay</li>
            </ul>
          </div>
          <div class="sidebar-card">
            <h4>Pipeline Stages</h4>
            <ul>
              <li><strong>Stage 1:</strong> Data Practices</li>
              <li><strong>Stage 2:</strong> Policy Goals / Metrics</li>
              <li><strong>Stage 3:</strong> Model / System</li>
              <li><strong>Stage 4:</strong> Workflow Use</li>
            </ul>
          </div>
          <div class="sidebar-card">
            <h4>Ethical Dimensions</h4>
            <ul>
              <li>Autonomy &mdash; manipulated choice</li>
              <li>Harm &mdash; mental health, addiction</li>
              <li>Rights &mdash; consent violations</li>
              <li>Accountability &mdash; opaque systems</li>
            </ul>
          </div>
          <div class="sidebar-card">
            <h4>Key References</h4>
            <ul>
              <li>Zuboff (2019), <em>The Age of Surveillance Capitalism</em></li>
              <li>Center for Humane Technology (2021)</li>
              <li>Frankfurt (1971), "Freedom of the Will"</li>
            </ul>
          </div>
          <div class="sidebar-card">
            <h4>All Projects</h4>
            <div class="sidebar-nav">
              <a href="bias-discrimination.html">Bias &amp; Discrimination</a>
              <a href="labor-automation.html">Labor &amp; Automation</a>
              <a href="intellectual-property.html">Intellectual Property</a>
              <a href="misinformation-deepfakes.html">Misinformation &amp; Deepfakes</a>
              <a href="ethical-obligations-ai.html">Moral Status of AI</a>
              <a href="power-monopoly.html">Power &amp; Monopoly</a>
              <a href="safety-security.html">Safety &amp; Security</a>
              <a href="environmental-impact.html">Environmental Impact</a>
              <a href="privacy-surveillance.html">Privacy &amp; Consent</a>
            </div>
          </div>
        </aside>
      </div>
    </div>
  </section>

  <footer class="site-footer">
    <div class="footer-inner">
      <div class="footer-col">
        <div class="footer-logos">
          <img src="images/ethiscope-logo.png" alt="Ethiscope Logo">
          <img src="images/ai-week-logo.png" alt="AI Week 2026">
        </div>
        <p>A PHIL 388 project exploring the ethical dimensions of artificial intelligence.</p>
      </div>
      <div class="footer-col">
        <h4>Projects</h4>
        <ul>
          <li><a href="bias-discrimination.html">Bias &amp; Discrimination</a></li>
          <li><a href="labor-automation.html">Labor &amp; Automation</a></li>
          <li><a href="intellectual-property.html">Intellectual Property</a></li>
          <li><a href="human-agency.html">Human Agency</a></li>
          <li><a href="misinformation-deepfakes.html">Misinformation &amp; Deepfakes</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <h4>More Projects</h4>
        <ul>
          <li><a href="ethical-obligations-ai.html">Moral Status of AI</a></li>
          <li><a href="power-monopoly.html">Power &amp; Monopoly</a></li>
          <li><a href="safety-security.html">Safety &amp; Security</a></li>
          <li><a href="environmental-impact.html">Environmental Impact</a></li>
          <li><a href="privacy-surveillance.html">Privacy &amp; Consent</a></li>
        </ul>
      </div>
    </div>
    <div class="footer-bottom">
      <p>Widener University &middot; PHIL 388: Ethics of Artificial Intelligence &middot; Dr. Tom Wilk &middot; Spring 2026</p>
    </div>
  </footer>

  <script src="easter-egg.js"></script>
</body>
</html>
